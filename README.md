
## 动机与成效

从第一期的 resnet 分类模型，到第二期的蒙特卡洛方法，如今已是第三期时序差分 Sarsa，一路走来，收获很多。一方面对动作游戏有了更深层次的理解，另外一方面也学到了一些强化学习方面的知识。

在蒙特卡洛项目中，state 的定义由聚类模型预测产生，从业务层面无法解释，所以在本次项目中，我们尝试重新定义 state。

最终训练出来的 policy，与蒙特卡洛项目中的 policy 非常相似，稳定性非常的强，绝大部分时间都在防御，但是发起攻击的时机比蒙特卡洛 policy 要略多一些，所以整个战斗过程也稍快一些。攻击多了之后也产生了负面影响，比如受伤的概率也变大了。果然是做的越多，错的就越多。


## 演示视频

三周目白金的时序差分 Sarsa 狼 再战 稀世强者苇名弦一郎

https://www.bilibili.com/video//

## 游戏设置

- steam 开始游戏
- 游戏设置：图像设定 -- 屏幕模式设置为窗口，游戏分辨率调整为`1280*720`
- 游戏设置：按键设置 -- 使用道具设置为按键`p`, 动作（长按）吸引设置为按键 `e`. 重置视角/固定目标设置为`q`，跳跃键为`f`(未用到)，垫步/识破键为`空格`，鼠标`左键`攻击，`右键`防御
	如果需要改变按键，可以在 `config/actions_conf.yaml` 文件中依次修改各个动作中的按键.
- 把 `伤药葫芦` 设置为第一个快捷使用的道具, 最好只设置这一个道具
- 游戏设置：网络设定 -- 标题画面启动设定为`离线游玩`，目的是为了关闭残影，满地的残影烦死了。
- 保持游戏窗口在最上层，不要最小化或者被其它的全屏窗口覆盖。

## 思路与问题

虽然算法从蒙特卡洛变成了时序差分，但状态 state 的定义依然是一个比较关键的问题。

直观来讲，可以把 boss 当前正在使用的招式作为当前的状态，只要识别出 boss 的招式，player 即可作出相应的应对，这应该也是人类新手玩家的打法。弦一郎一阶段的招式大概不超过 10 种，看起来又是一个分类或者聚类问题。

可是，所谓的招式是连续多帧的动作组合，一帧动作截个图做分类模型我们经常做，多帧动作截图的集合也可以做分类模型吗？值得尝试一下。

在此项目临近结束，补充 readme 的时候，我们才突然意识到，多个连续动作的截图组合在一起，可以认为是一个视频，业界似乎也有视频分类模型，但这次来不及尝试了，代码都写完了。。。下次吧，也不知道我们的破显卡能不能带得动。

这次做的仍然是单帧动作截图的分类，与之前图片分类项目有所不同。之前项目中的 X 是游戏中的截图，y 是截图产生时，玩家按下的按键(左键攻击、右键防御、空格识破等）；在此项目中，X 仍旧是游戏中的截图，y 是游戏结束之后，人工打的标记，把同一个招式的多个连续动作的截图归于一类，放到同一个文件夹中。

当然，我们也没有把全部招式都标记出来，感觉有点累，只标记了如下的四个关键动作，主要是因为这四个动作只要正确处理，都可以打出伤害，危机危机，危险里面蕴含着机会。

1. 突刺危，boss 先是起跳下击，接后撤突刺。处理方法为防御 + 识破 + 攻击。
2. 擒拿危。处理方法为攻击打断 + 攻击。
3. 飞渡浮舟，也就是 2 连击 + 7 连击，处理方法为防御 + 攻击。
4. 突刺危的变种，boss 原地后撤突刺，出招比较隐蔽。处理方法为识破 + 攻击。

一开始，上面四个关键动作的截图收集的很全面，比如飞渡浮舟，从头到尾把招式中的几十帧都收集下来了，后来发现没啥必要，我们真正需要关注的是招式结尾的几帧。

这四个关键招式外的动作截图都做为分类 0，处理方法为防御。

数据集必然是非常不均衡的，分类 0 截图的数量极大，所以我们对分类 0 做了采样，只录制了一点点数据。

各个分类中的图片数量如下：

`[576, 393, 298, 147, 167]`

这次的分类模型在测试集上的准确率极高，达到了 `0.974`
各个分类的准确率：`eval accuracy:  {'0': 0.968944099378882, '1': 0.970873786407767, '2': 0.9259259259259259, '3': 0.975609756097561, '4': 0.9787234042553191}`

到游戏里面尝试了这个分类模型，发现分类 0 经常会被误识别到其它分类里面去（分类 4 是重灾区），原因是 boss 的其它招式的部分动作确实与上面定义的四个招式中的部分动作有点相似。当然偶尔错误的预测也会带来一些惊喜，比如意外的出刀打断了 boss 的某个招式。

为了解决这个问题，我们悟出了 `信号强度` 的概念：同一招式中连续多个帧的分类应该是相同的，在预测时，连续帧相同分类出现的次数可以称为`信号强度`，信号越强，表示对该招式的预测越准确。
稍微尝试了一下，发现只需要有连续三帧预测出来的分类相同，就可以显著缓解上述招式误识别的问题，但分类 4 仍然效果不佳。

总结一下，我们做的其实是如下的数据转化：

`实时截图 image -> 分类 class(不准确) -> 状态 state(相对准确) -> 动作 action(需要探索) `

看起来，似乎只要针对各个 state 定义一些处理规则，就可以应对 boss 的各种招式了，但有些 action 也值得探索一下，比如识破之后，出刀一次还是两次，抑或是三次四次五次。人类玩家在连续出刀的过程中可以实时观察 boss 状态来决定是否中止连招，但程序却需要事先明确连续出刀几次，因为程序发起的连招动作目前还无法在打击过程中随时中止，必须全部打完才能换下一个动作，难度主要在于 boss 状态的判断。

有没有一种在做 grid search 的感觉。

以上的五个分类 0-4 可以作为状态 0-4，再定义两个新的 state：5 与 6 及其处理方法：
state-5 指的是 player 受到了伤害，处理方法为向后垫步，如果是躺在地上则会快速起身。
state-6 指的是 boss 受到了伤害且 player hp < 60，此时应该去喝血瓶。

这两个 states 不需要探索 action，直接使用预定义的处理规则即可。


从最终结果来看，这种状态定义方式已经比较合理了，但不够全面，未能完整反映出游戏中的真实情况。

最后，训练出来的 policy 在某一个 episode 中表现不一定好，但如果战斗多个 episode，那么整体结果一定是比较好的。 就类似于股票策略，它在某些交易的时候可能是亏损的，但是长期坚持下去一定是会盈利的。

## 如何训练

- 确认环境

`python debug_display_game_info.py`

会在 assets 目录中生成 debug_ui_elements.jpg，该图片中会绘制游戏屏幕截图中的各个 window 区域

同时还会弹出一个小的 tk 窗口实时显示 player 与 boss 的 hp，这个功能得感谢原作者。


- 测试：执行某个或者某几个动作

`python test.py`


- 收集数据

`python data_collector.py`

按 ] 键开始收集;

所谓的收集，就是在玩家打游戏的过程中，定期对游戏屏幕的 boss 区域进行截屏(301x301)，以 list 的形式保存在内存中。

一个 episode 结束的时候，内存中的数据集会被保存到硬盘目录 `images/original` 中。

按 Backspace 键，退出程序

如果在命令行中使用了 `--new` 参数，会首先清除 images/original 目录


- 人工打标记

把 images/original 中的图片按照不同的招式挑选出来放到 `images/move` 目录中的 0-4 子目录中。

在仓库中我们已经提供了自己的 move 目录的压缩包 move.zip


- 训练分类模型

`python train_classifier.py` 

它会切分训练集与测试集并训练一个 resnet18 分类模型，并在测试集上评估模型的效果。 


- 训练 Sarsa policy

`python train.py`

默认会加载 checkpoint 文件中的训练相关信息以及 Q 和 N，然后在此基础上进行训练。

进入游戏后，按 q 键锁定 boss，按 ] 键开始正式的训练。

如果在命令行中使用了 `--new` 参数，会从第 0 个 episode 开始重新训练。

每一个 episode 结束之后，把 Q 与 N 保存到 checkpoint 文件中。

时序差分并不需要 N，我们把 N 加上了，主要用于观察Q(s)中每个 action 的出现次数。


- 查看 Q与 N

`python checkpoint.py`



## 预测

进入游戏，

在 cmd 窗口中运行：
```
python main.py 
```

等待模型加载完，tk窗口出现，

按 q 键锁定 boss

按 ] 键, 就会针对敌方的出招自动做出预测动作了。

再次按下 ] 键，会停止预测。

按 Backspace 键，退出程序。


## 人工备份

模型的训练过程与结果主要涉及到如下的几个文件：
- images/original	            收集到的截屏图像文件
- images/move   人工标记后的图像文件
- checkpoint.json		记录了当前是哪个 episode，以及完成训练时的时间。
- checkpoint.pkl		存储了 Q 与 N
- model.resnet.v2   分类模型


如果要训练新模型的话，可能需要对老模型的这些数据进行备份。


## 大部分代码和思路来自以下网址，感谢他们

- https://github.com/XR-stb/DQN_WUKONG
- https://github.com/analoganddigital/DQN_play_sekiro
- https://github.com/Sentdex/pygta5
- https://github.com/RongKaiWeskerMA/sekiro_play

- https://www.lapis.cafe/posts/ai-and-deep-learning/%E4%BD%BF%E7%94%A8resnet%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B
- https://blog.csdn.net/qq_36795658/article/details/100533639
- https://blog.csdn.net/Guo_Python/article/details/134922730

- X 图片 y 按键 分类项目 https://github.com/XuLvXiu/sekiro_classifier_ai
- 蒙特卡洛项目 https://github.com/XuLvXiu/sekiro_rl_mc_ai



